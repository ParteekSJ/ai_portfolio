<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313/favicon_io//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon_io//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon_io//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313/favicon_io//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/favicon_io//apple-touch-icon.png">

<meta name="description" content="" />

<title>
    
     | Parteek Jamwal
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/shannon-entropy/" />

<meta property="og:url" content="http://localhost:1313/posts/shannon-entropy/">
  <meta property="og:site_name" content="Parteek Jamwal">
  <meta property="og:title" content="Parteek Jamwal">
  <meta property="og:description" content="It quantifies the amount of uncertainty or randomness in a random variable or a probability distribution. In essence, Shannon Entropy measures the expected amount of information produced by a stochastic source of data.
For a discrete RV $X$ that can take in on values ${x_1,x_2,\dots,x_n}$ with corresponding probabilities ${p_1,p_2,\dots,p_n}$, the Shannon Entropy $H(X)$ is defined as follows: $$ H(X) = -\sum_{i=1}^np_i \log_b p_i $$
$H(X)$ is the entropy of the random variable $X$.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">













<link rel="stylesheet" href="/assets/combined.min.5d549282aa71faacfceb2e30934b742b30ac25d0b3a9b86aa876fd5ac00088bd.css" media="all">



  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Parteek Jamwal</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/papers" >
                /papers
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/projects" >
                /projects
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/shannon-entropy/"></a>
</div>



<div >

  <div class="single-intro-container">

    

    <h1 class="single-title"></h1>
    

    

    <p class="single-readtime">
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>It quantifies the amount of uncertainty or randomness in a random variable or a probability distribution. In essence, Shannon Entropy measures the expected amount of information produced by a stochastic source of data.</p>
<p>For a discrete RV $X$ that can take in on values ${x_1,x_2,\dots,x_n}$ with corresponding probabilities ${p_1,p_2,\dots,p_n}$, the Shannon Entropy $H(X)$ is defined as follows: $$ H(X) = -\sum_{i=1}^np_i \log_b p_i $$</p>
<ul>
<li>$H(X)$ is the entropy of the random variable $X$.</li>
<li>$p_i$ is the probability of the $i$&lsquo;th outcome.</li>
<li>$\log_b$ is simply the logarithm with base $b$. Common choices are
<ul>
<li>$b=2$: Entropy measured in bits.</li>
<li>$b=e$: Entropy measured in nats (natural units).</li>
<li>$b=10$: Entropy measured in bans or dits.</li>
</ul>
</li>
</ul>
<h2 id="intuition-behind-shannon-entropy"><strong>Intuition Behind Shannon Entropy</strong></h2>
<ul>
<li><strong>Uncertainty Measurement</strong>: Entropy quantifies the average level of &ldquo;surprise&rdquo; or &ldquo;uncertainty&rdquo; inherent in the variable&rsquo;s possible outcomes.</li>
<li><strong>Information Content</strong>: Higher entropy means more unpredictability and thus more information content per observation.</li>
<li><strong>Uniform Distribution</strong>: The entropy is maximized when all outcomes are equally likely (maximum uncertainty).</li>
<li><strong>Deterministic Outcome</strong>: If one outcome has a probability of 1 (no uncertainty), the entropy is zero.</li>
</ul>
<h2 id="properties-of-shannon-entropy">Properties of Shannon-Entropy</h2>
<ol>
<li>Non-Negativity: $H(X) \geq 0$</li>
<li>Additivity for Independent Variables: If $X$ and $Y$ are random variables then $$ H(X,Y) = H(X) + H(Y) $$</li>
<li>Joint Entropy: For joint random variables $X$ and $Y$: $$ H(X,Y) = H(X) + H(Y|X) $$</li>
<li>Chain Rule: Entropy can be decomposed into a sum of conditional entropies: $$ H(X_1,X_2,&hellip;,X_n)=\sum_{i=1}^nH(X_i|X_1,&hellip;,X_{i-1}) $$</li>
</ol>
<blockquote>
<p>In ML, we usually use natural logarithm, and the entropy unit is <code>nit</code>.</p>
</blockquote>
<h2 id="examples">Examples</h2>
<ol>
<li>Fair Coin Toss: A fair coin has 2 possible outcomes each with a probability of $0.5$. Hence, the entropy is as follows: $$
\begin{aligned}
H(X) &amp;= -\sum_{i=1}^n p_i \log_2 p_i \
&amp;= -\left[0.5 \log_2 0.5 + 0.5 \log_2 0.5\right] \
&amp;= 1 \text{ bit}
\end{aligned}
$$</li>
<li>Biased Coin Toss: A coin with $P(H)=0.8$ and $P(T)=0.2$ has the following entropy: $$ \begin{aligned}H(X)=-(0.8\log_20.8+0.2\log_20.2)\approx0.7219\mathrm{~bits} \end{aligned}  $$The entropy is less than 1 bit because the outcome is more predictable.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a90d91">import</span> <span style="color:#000">numpy</span> <span style="color:#a90d91">as</span> <span style="color:#000">np</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Define the probabilities</span>
</span></span><span style="display:flex;"><span><span style="color:#000">probabilities</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">array</span>([<span style="color:#1c01ce">0.5</span>, <span style="color:#1c01ce">0.5</span>])
</span></span><span style="display:flex;"><span><span style="color:#000">probabilities_biased_coin</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">array</span>([<span style="color:#1c01ce">0.8</span>, <span style="color:#1c01ce">0.2</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Ensure the probabilities sum to 1</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">assert</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">isclose</span>(<span style="color:#000">probabilities</span><span style="color:#000">.</span><span style="color:#000">sum</span>(), <span style="color:#1c01ce">1</span>), <span style="color:#c41a16">&#34;Probabilities must sum to 1.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Compute the Shannon entropy</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">def</span> <span style="color:#000">shannon_entropy</span>(<span style="color:#000">p</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#177500"># Filter out zero probabilities to avoid log(0)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">p</span> <span style="color:#000">=</span> <span style="color:#000">p</span>[<span style="color:#000">p</span> <span style="color:#000">&gt;</span> <span style="color:#1c01ce">0</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#a90d91">return</span> <span style="color:#000">-</span><span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">sum</span>(<span style="color:#000">p</span> <span style="color:#000">*</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">log2</span>(<span style="color:#000">p</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#000">entropy</span> <span style="color:#000">=</span> <span style="color:#000">shannon_entropy</span>(<span style="color:#000">probabilities</span>)
</span></span><span style="display:flex;"><span><span style="color:#a90d91">print</span>(<span style="color:#c41a16">f</span><span style="color:#c41a16">&#34;Shannon Entropy: </span><span style="color:#c41a16">{</span><span style="color:#000">entropy</span><span style="color:#c41a16">:</span><span style="color:#c41a16">.4f</span><span style="color:#c41a16">}</span><span style="color:#c41a16"> bits&#34;</span>)
</span></span></code></pre></div><p>A fair dice system&rsquo;s entropy is $$H(p) = -6(1/6\times \log_2 (1/6)) = 2.585$$</p>
<h2 id="visualization-of-entropy-of-a-binary-distribution">Visualization of Entropy of a Binary Distribution</h2>
<p>













<figure class=" img-dark%23small">

    <div>
        <img loading="lazy" alt="alt text" src="/assets/posts/shannon-entropy/shannon-entropy.png#dark%23small">
    </div>

    
    <div class="caption-container">
        <figcaption> caption text </figcaption>
    </div>
    
</figure></p>
<p>In this plot we can see the entropy of a binary distribution. The entropy is the highest when both the events are equally likely, i.e., $Pr(A)=Pr(B)=0.5$. As the probability of one event happening increases, the entropy decreases which is evident from the graph.</p>
<p>Code:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a90d91">import</span> <span style="color:#000">matplotlib.pyplot</span> <span style="color:#a90d91">as</span> <span style="color:#000">plt</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Probabilities ranging from 0.01 to 0.99</span>
</span></span><span style="display:flex;"><span><span style="color:#000">p_values</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">linspace</span>(<span style="color:#1c01ce">0.01</span>, <span style="color:#1c01ce">0.99</span>, <span style="color:#1c01ce">100</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">entropies</span> <span style="color:#000">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">for</span> <span style="color:#000">p</span> <span style="color:#000">in</span> <span style="color:#000">p_values</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#000">q</span> <span style="color:#000">=</span> <span style="color:#1c01ce">1</span> <span style="color:#000">-</span> <span style="color:#000">p</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000">entropy</span> <span style="color:#000">=</span> <span style="color:#000">shannon_entropy</span>(<span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">array</span>([<span style="color:#000">p</span>, <span style="color:#000">q</span>]))
</span></span><span style="display:flex;"><span>    <span style="color:#000">entropies</span><span style="color:#000">.</span><span style="color:#000">append</span>(<span style="color:#000">entropy</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">figure</span>(<span style="color:#000">figsize</span><span style="color:#000">=</span>(<span style="color:#1c01ce">8</span>, <span style="color:#1c01ce">5</span>))
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">plot</span>(<span style="color:#000">p_values</span>, <span style="color:#000">entropies</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">title</span>(<span style="color:#c41a16">&#39;Entropy of a Binary Distribution&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">xlabel</span>(<span style="color:#c41a16">&#39;Probability of Outcome A (p)&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">ylabel</span>(<span style="color:#c41a16">&#39;Entropy (bits)&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">grid</span>(<span style="color:#a90d91">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#000">plt</span><span style="color:#000">.</span><span style="color:#000">show</span>()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Entropy is highest when [p,q] = [0.5, 0.5]</span>
</span></span></code></pre></div><h3 id="explanation"><strong>Explanation</strong>:</h3>
<ul>
<li><strong>Binary Distribution</strong>: We consider a binary random variable with probabilities $P(A)=p$ and $P(B)=1−p$.</li>
<li><strong>Entropy Curve</strong>: The entropy is maximal when $p=0.5$ (most uncertainty) and minimal when $p=0$ or $p=1$ (no uncertainty).</li>
</ul>

    
  </div>

  


  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/posts/kv-cache_attention/">
                        What is KV-Cache &amp; Attention?
                    </a>
                </div>
                <div class="single-pagination-text">→</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      









    </footer>
    
  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>