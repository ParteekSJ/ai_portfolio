<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313/favicon_io//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon_io//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon_io//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313/favicon_io//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/favicon_io//apple-touch-icon.png">

<meta name="description" content="An LSM Tree overview and Java implementation." />

<title>
    
    What is KL Divergence? | Parteek Jamwal üêâ
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/kl-divergence-copy/" />

<meta property="og:url" content="http://localhost:1313/posts/kl-divergence-copy/">
  <meta property="og:site_name" content="Parteek Jamwal üêâ">
  <meta property="og:title" content="What is KL Divergence?">
  <meta property="og:description" content="An LSM Tree overview and Java implementation.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-12T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-08-12T00:00:00+00:00">
    <meta property="article:tag" content="Database">
    <meta property="article:tag" content="Java">













<link rel="stylesheet" href="/assets/combined.min.45a02fa7ead823e26580f0d23e8ef6fec9537e0a48c29380b61392e60aaaa15c.css" media="all">



  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Parteek Jamwal üêâ</a>
    </h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/papers" >
                /papers
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/projects" >
                /projects
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        





<div class="breadcrumbs">
    
    <a href="/">Home</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a href="/posts/">Posts</a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/posts/kl-divergence-copy/">What is KL Divergence?</a>
</div>



<div  class="autonumber" >

  <div class="single-intro-container">

    

    <h1 class="single-title">What is KL Divergence?</h1>
    
    <p class="single-summary">An LSM Tree overview and Java implementation.</p>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-08-12T00:00:00&#43;00:00">August 12, 2024</time>
      

      
      &nbsp; ¬∑ &nbsp;
      4 min read
      
    </p>

  </div>

  

  

  
  <aside class="toc">
    <p><strong>Table of contents</strong></p>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#mathematical-formulation">Mathematical Formulation</a>
      <ul>
        <li><a href="#discrete-distribution">Discrete Distribution</a></li>
        <li><a href="#continuous-distribution">Continuous Distribution</a></li>
      </ul>
    </li>
    <li><a href="#intuition">Intuition</a></li>
    <li><a href="#properties-of-kl-divergence">Properties of KL Divergence</a></li>
    <li><a href="#examples">Examples</a>
      <ul>
        <li><a href="#discrete-example">Discrete Example</a></li>
        <li><a href="#continuous-example">Continuous Example</a></li>
        <li><a href="#python-implementation"><strong>Python Implementation</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
  </aside>
  

  

  <div class="single-content">
    <p>













<figure class=" img-dark%23small">

    <div>
        <img loading="lazy" alt="MMR Architecture." src="/assets/projects/aeroengine-blade-project-assets/output_video_V2.gif#dark%23small">
    </div>

    
    <div class="caption-container">
        <figcaption> MMR Architecture </figcaption>
    </div>
    
</figure></p>
<p>In the case where there are two distribution functions $p$ and $q$ for the same random variable $X$, their log-likelihood ratio is a function $f(x)=\ln(p(x) / q(x))$.</p>
<blockquote>
<p>Expected Value of the log-likelihood ratio is called Kullback-Liebler Divergence.</p>
</blockquote>
<p>KL Divergence measures the amount of information lost when one probability distribution $Q$ is used to approximate another probability distribution $P$. It is a ==non-symmetric measure==, i.e., $$ \mathrm{KL}(Q|P) \neq \mathrm{KL}(P|Q) $$</p>
<h2 id="mathematical-formulation">Mathematical Formulation</h2>
<h3 id="discrete-distribution">Discrete Distribution</h3>
<p>For two discrete distributions $P$ and $Q$ defined over the same probability space $\mathcal{X}$, the KL Divergence from $Q$ to $P$ is defined as follows: $$ \mathrm{KL}(P|Q) = \sum_{x\in\mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right) $$</p>
<h3 id="continuous-distribution">Continuous Distribution</h3>
<p>For two continuous probability distributions with probability density functions $p(x)$ and $q(x)$ over the same domain $\mathcal{X}$, the KL Divergence from $Q$ to $P$ is defined as follows: $$ \mathrm{KL}(P|Q) = \int_{-\infty}^{\infty}  p(x) \log \left( \frac{p(x)}{q(x)} \right) $$</p>
<!-- raw HTML omitted -->
<h2 id="intuition">Intuition</h2>
<ol>
<li>Information Gained: KL Divergence measures the expected amount of extra bits required code samples from $P$ when using a code optimized for $Q$ instead of the true distribution $P$.</li>
<li>Relative Entropy: It quantifies the &ldquo;<strong>distance</strong>&rdquo; or difference between 2 distributions, although it is not a true metric because it isn&rsquo;t symmetric and doesn&rsquo;t satisfy the triangle inequality property.</li>
<li>Direction Matters: $\mathrm{KL}(P|Q)$ can be interpreted as how well $Q$ approximates the distribution $P$, not the around way around.</li>
</ol>
<h2 id="properties-of-kl-divergence">Properties of KL Divergence</h2>
<ol>
<li>Non-Negativity (Gibbs Inequality): $\mathrm{KL}(P|Q) \geq 0$. Equality holds iff $P=Q$ almost everywhere.</li>
<li>Non Symmetry: $\mathrm{KL}(P|Q) \neq \mathrm{KL}(Q|P)$</li>
<li>Additivity for Independent Distributions: If $P(x,y)=P(x)P(y)$ and $Q(x,y)=Q(x)Q(y)$ then $$ \mathrm{KL}(P(x,y) | Q(x,y))=\mathrm{KL}(P(x) | Q(x))+\mathrm{KL}(P(y) | Q(y)) $$</li>
<li>Relation to Entropy: KL Divergence can be expressed in terms of entropy: $$ \mathrm{KL}(P\parallel Q)=H(P,Q)-H(P) $$ where
<ul>
<li>$H(P)$ is the entropy of the distribution $P$.</li>
<li>$H(P,Q)$ is the cross-entropy between the distributions $P$ and $Q$.</li>
</ul>
</li>
</ol>
<h2 id="examples">Examples</h2>
<h3 id="discrete-example">Discrete Example</h3>
<p>Consider 2 discrete distributions $P$ and $Q$ over the same set $\mathcal{X} ={A,B,C}$:<br>
$$
\begin{gather*}
P = \begin{cases}
P(A) = 0.4 \
P(B) = 0.35 \
P(C) = 0.25
\end{cases} \\
Q = \begin{cases}
Q(A) = 0.5 \
Q(B) = 0.3 \
Q(C) = 0.2
\end{cases}
\end{gather*}
$$</p>
<p>Computing $\text{KL}(P|Q)$, we get
$$
\begin{aligned}
\mathrm{KL}(P \parallel Q) &amp;= 0.4 \log\left( \frac{0.4}{0.5} \right) + 0.35 \log\left( \frac{0.35}{0.3} \right) + 0.25 \log\left( \frac{0.25}{0.2} \right)
\end{aligned}
$$</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>$$\mathrm{KL}(P\parallel Q)\approx-0.0892+0.0542+0.0558=0.0208\mathrm{~nats}$$</p>
<h3 id="continuous-example">Continuous Example</h3>
<p>For continuous distributions, consider $P$ and $Q$ are Gaussian distributions:  $$P=\mathcal{N}(\mu_p,\sigma_p^2)\quad\mathrm{and}\quad Q=\mathcal{N}(\mu_q,\sigma_q^2)$$. The KL Divergence between them is as follows: $$\mathrm{KL}(P\parallel Q)=\log\left(\frac{\sigma_q}{\sigma_p}\right)+\frac{\sigma_p^2+(\mu_p-\mu_q)^2}{2\sigma_q^2}-\frac12$$</p>
<!-- raw HTML omitted -->
<p>$$\mathrm{KL}[P\mid\mid Q]=\frac12\left[\frac{(\mu_2-\mu_1)^2}{\sigma_2^2}+\frac{\sigma_1^2}{\sigma_2^2}-\ln\frac{\sigma_1^2}{\sigma_2^2}-1\right].$$</p>
<p>Example: Let $P=\mathcal{N}(0,1^2)$ and $P=\mathcal{N}(1,2^2)$:
$$\mathrm{KL}(P\parallel Q)=\log\left(\frac{2}{1}\right)+\frac{1+(0-1)^{2}}{2\times4}-\frac{1}{2}=\log(2)+\frac{2}{8}-\frac{1}{2}\approx0.6931+0.25-0.5=0.4431\mathrm{~nats}$$</p>
<hr>
<h3 id="python-implementation"><strong>Python Implementation</strong></h3>
<p>Let&rsquo;s explore how to compute and visualize KL Divergence in Python using both discrete and continuous distributions.</p>
<h4 id="computing-kl-divergence"><strong>Computing KL Divergence</strong></h4>
<p><strong>1. Discrete KL Divergence</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a90d91">import</span> <span style="color:#000">numpy</span> <span style="color:#a90d91">as</span> <span style="color:#000">np</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">from</span> <span style="color:#000">scipy.stats</span> <span style="color:#a90d91">import</span> <span style="color:#000">entropy</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Define the discrete distributions P and Q</span>
</span></span><span style="display:flex;"><span><span style="color:#000">P</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">array</span>([<span style="color:#1c01ce">0.4</span>, <span style="color:#1c01ce">0.35</span>, <span style="color:#1c01ce">0.25</span>])
</span></span><span style="display:flex;"><span><span style="color:#000">Q</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">array</span>([<span style="color:#1c01ce">0.5</span>, <span style="color:#1c01ce">0.3</span>, <span style="color:#1c01ce">0.2</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Compute KL Divergence using scipy</span>
</span></span><span style="display:flex;"><span><span style="color:#000">kl_div</span> <span style="color:#000">=</span> <span style="color:#000">entropy</span>(<span style="color:#000">P</span>, <span style="color:#000">Q</span>)  <span style="color:#177500"># By default, uses natural logarithm (nats)</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">print</span>(<span style="color:#c41a16">f</span><span style="color:#c41a16">&#34;KL Divergence (P || Q): </span><span style="color:#c41a16">{</span><span style="color:#000">kl_div</span><span style="color:#c41a16">:</span><span style="color:#c41a16">.4f</span><span style="color:#c41a16">}</span><span style="color:#c41a16"> nats&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># For bits, change the base using the &#39;base&#39; parameter or convert manually</span>
</span></span><span style="display:flex;"><span><span style="color:#000">kl_div_bits</span> <span style="color:#000">=</span> <span style="color:#000">kl_div</span> <span style="color:#000">/</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">log</span>(<span style="color:#1c01ce">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#a90d91">print</span>(<span style="color:#c41a16">f</span><span style="color:#c41a16">&#34;KL Divergence (P || Q): </span><span style="color:#c41a16">{</span><span style="color:#000">kl_div_bits</span><span style="color:#c41a16">:</span><span style="color:#c41a16">.4f</span><span style="color:#c41a16">}</span><span style="color:#c41a16"> bits&#34;</span>)
</span></span></code></pre></div><p><strong>Output</strong>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>KL Divergence (P || Q): 0.0208 nats
</span></span><span style="display:flex;"><span>KL Divergence (P || Q): 0.0301 bits
</span></span></code></pre></div><p><strong>2. Continuous KL Divergence</strong>
For continuous distributions, especially Gaussian, we can compute the KL Divergence analytically or numerically.</p>
<p><strong>Example with Gaussian Distributions</strong>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a90d91">from</span> <span style="color:#000">scipy.stats</span> <span style="color:#a90d91">import</span> <span style="color:#000">norm</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Parameters for P and Q</span>
</span></span><span style="display:flex;"><span><span style="color:#000">mu_p</span>, <span style="color:#000">sigma_p</span> <span style="color:#000">=</span> <span style="color:#1c01ce">0</span>, <span style="color:#1c01ce">1</span>
</span></span><span style="display:flex;"><span><span style="color:#000">mu_q</span>, <span style="color:#000">sigma_q</span> <span style="color:#000">=</span> <span style="color:#1c01ce">1</span>, <span style="color:#1c01ce">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Analytical KL Divergence for Gaussian</span>
</span></span><span style="display:flex;"><span><span style="color:#000">kl_div</span> <span style="color:#000">=</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">log</span>(<span style="color:#000">sigma_q</span> <span style="color:#000">/</span> <span style="color:#000">sigma_p</span>) <span style="color:#000">+</span> (<span style="color:#000">sigma_p</span><span style="color:#000">**</span><span style="color:#1c01ce">2</span> <span style="color:#000">+</span> (<span style="color:#000">mu_p</span> <span style="color:#000">-</span> <span style="color:#000">mu_q</span>)<span style="color:#000">**</span><span style="color:#1c01ce">2</span>) <span style="color:#000">/</span> (<span style="color:#1c01ce">2</span> <span style="color:#000">*</span> <span style="color:#000">sigma_q</span><span style="color:#000">**</span><span style="color:#1c01ce">2</span>) <span style="color:#000">-</span> <span style="color:#1c01ce">0.5</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">print</span>(<span style="color:#c41a16">f</span><span style="color:#c41a16">&#34;KL Divergence (P || Q): </span><span style="color:#c41a16">{</span><span style="color:#000">kl_div</span><span style="color:#c41a16">:</span><span style="color:#c41a16">.4f</span><span style="color:#c41a16">}</span><span style="color:#c41a16"> nats&#34;</span>)
</span></span></code></pre></div><p><strong>Output</strong>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>KL Divergence (P || Q): 0.4431 nats
</span></span></code></pre></div><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><strong>Example</strong>: Compute KL Divergence between two normal distributions using numerical integration.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a90d91">from</span> <span style="color:#000">scipy.integrate</span> <span style="color:#a90d91">import</span> <span style="color:#000">quad</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Define the probability density functions</span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">def</span> <span style="color:#000">p</span>(<span style="color:#000">x</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#a90d91">return</span> <span style="color:#000">norm</span><span style="color:#000">.</span><span style="color:#000">pdf</span>(<span style="color:#000">x</span>, <span style="color:#000">mu_p</span>, <span style="color:#000">sigma_p</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a90d91">def</span> <span style="color:#000">log_ratio</span>(<span style="color:#000">x</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#a90d91">return</span> <span style="color:#000">norm</span><span style="color:#000">.</span><span style="color:#000">pdf</span>(<span style="color:#000">x</span>, <span style="color:#000">mu_p</span>, <span style="color:#000">sigma_p</span>) <span style="color:#000">*</span> (<span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">log</span>(<span style="color:#000">norm</span><span style="color:#000">.</span><span style="color:#000">pdf</span>(<span style="color:#000">x</span>, <span style="color:#000">mu_p</span>, <span style="color:#000">sigma_p</span>)) <span style="color:#000">-</span> <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">log</span>(<span style="color:#000">norm</span><span style="color:#000">.</span><span style="color:#000">pdf</span>(<span style="color:#000">x</span>, <span style="color:#000">mu_q</span>, <span style="color:#000">sigma_q</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#177500"># Compute KL Divergence via numerical integration</span>
</span></span><span style="display:flex;"><span><span style="color:#000">kl_div_num</span>, <span style="color:#000">_</span> <span style="color:#000">=</span> <span style="color:#000">quad</span>(<span style="color:#000">log_ratio</span>, <span style="color:#000">-</span><span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">inf</span>, <span style="color:#000">np</span><span style="color:#000">.</span><span style="color:#000">inf</span>)
</span></span><span style="display:flex;"><span><span style="color:#a90d91">print</span>(<span style="color:#c41a16">f</span><span style="color:#c41a16">&#34;Numerical KL Divergence (P || Q): </span><span style="color:#c41a16">{</span><span style="color:#000">kl_div_num</span><span style="color:#c41a16">:</span><span style="color:#c41a16">.4f</span><span style="color:#c41a16">}</span><span style="color:#c41a16"> nats&#34;</span>)
</span></span></code></pre></div><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

    
  </div>

  


  

  
  

<div class="single-pagination">
    <hr />

    <div class="flex">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">‚Üê</div>
                <div class="single-pagination-text">
                    <a href="/posts/kl-divergence/">
                        What is KL Divergence?
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/posts/positional_encoding/">
                        Positional Embeddings
                    </a>
                </div>
                <div class="single-pagination-text">‚Üí</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      









<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>

    </footer>
    
  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>